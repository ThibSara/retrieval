{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from implicit.als import AlternatingLeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1A: LOAD YAHOO & COAT DATASET\n",
    "\n",
    "\n",
    "def load_yahoo_data():\n",
    "    \"\"\"\n",
    "    Load Yahoo! R3 dataset from predefined train, validation, and test files.\n",
    "    \"\"\"\n",
    "\n",
    "    train = pd.read_csv(\"./yahoo/trainset.csv\")\n",
    "    valid = pd.read_csv(\"./yahoo/validset.csv\")\n",
    "    test = pd.read_csv(\"./yahoo/testset.csv\")\n",
    "    train = train[['user_id', 'item_id', 'rating']]\n",
    "    valid = valid[['user_id', 'item_id', 'rating']]\n",
    "    test = test[['user_id', 'item_id', 'rating']]\n",
    "\n",
    "    train.columns = valid.columns = test.columns = ['user_id', 'item_id', 'rating']\n",
    "    return train, valid, test\n",
    "\n",
    "\n",
    "def load_coat_data():\n",
    "    \"\"\"\n",
    "    Load Coat dataset from predefined train, validation, and test files.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(\"./coat/trainset.csv\")\n",
    "    valid = pd.read_csv(\"./coat/validset.csv\")\n",
    "    test = pd.read_csv(\"./coat/testset.csv\")\n",
    "    train.columns = valid.columns = test.columns = ['user_id', 'item_id', 'rating']\n",
    "    return train, valid, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1B: SYNTHETIC DATASET GENERATION\n",
    "\n",
    "def generate_synthetic_data(num_users=100, num_items=50, num_ratings=500):\n",
    "    user_ids = np.random.randint(1, num_users + 1, num_ratings)\n",
    "    item_ids = np.random.randint(1, num_items + 1, num_ratings)\n",
    "    ratings = np.random.randint(1, 6, num_ratings)\n",
    "    return pd.DataFrame({'user_id': user_ids, 'item_id': item_ids, 'rating': ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: PROPENSITY SCORE COMPUTATION\n",
    "\n",
    "def compute_propensities(df):\n",
    "    item_popularity = df['item_id'].value_counts(normalize=True)\n",
    "    rating_distribution = df['rating'].value_counts(normalize=True)\n",
    "    df['popularity_propensity'] = df['item_id'].map(lambda x: item_popularity.get(x, 0.01))\n",
    "    df['positivity_propensity'] = df['rating'].map(lambda x: rating_distribution.get(x, 0.01))\n",
    "    df['multifactorial_propensity'] = df['popularity_propensity'] * df['positivity_propensity']\n",
    "    df['ips_weight'] = 1 / (df['multifactorial_propensity'] + 1e-6)\n",
    "    df['weighted_rating'] = df['rating'] * df['ips_weight']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3A: MATRIX FACTORIZATION WITH SVD & ALS\n",
    "\n",
    "def baseline_recommender(train, test):\n",
    "    item_means = train.groupby('item_id')['rating'].mean()\n",
    "    return test['item_id'].map(lambda x: item_means.get(x, 3))\n",
    "\n",
    "def svd_recommender(train, test, k=20, debias=False):\n",
    "    column = 'weighted_rating' if debias else 'rating'\n",
    "    user_means = train.groupby('user_id')[column].mean()\n",
    "    train['rating_normalized'] = train.apply(lambda x: x[column] - user_means.get(x['user_id'], 3), axis=1)\n",
    "    user_item_matrix = train.pivot_table(index='user_id', columns='item_id', values=column, aggfunc='mean', fill_value=0)\n",
    "    sparse_matrix = csr_matrix(user_item_matrix)\n",
    "    U, sigma, Vt = svds(sparse_matrix, k=k)\n",
    "    sigma = np.diag(sigma)\n",
    "    predicted_ratings = np.dot(np.dot(U, sigma), Vt)\n",
    "    \n",
    "    predictions = []\n",
    "    num_users, num_items = predicted_ratings.shape\n",
    "    \n",
    "    for _, row in test.iterrows():\n",
    "        user_idx = min(max(int(row['user_id']) - 1, 0), num_users - 1)\n",
    "        item_idx = min(max(int(row['item_id']) - 1, 0), num_items - 1)\n",
    "        predictions.append(predicted_ratings[user_idx, item_idx])\n",
    "    \n",
    "    return np.clip(predictions, 1, 5)\n",
    "\n",
    "def als_recommender(train, test, factors=20, debias=False):\n",
    "    column = 'weighted_rating' if debias else 'rating'\n",
    "    user_item_matrix = csr_matrix(train.pivot_table(index='user_id', columns='item_id', values=column, aggfunc='mean', fill_value=0))\n",
    "    model = AlternatingLeastSquares(factors=factors, iterations=20)\n",
    "    model.fit(user_item_matrix)\n",
    "    \n",
    "    predictions = []\n",
    "    num_users, num_items = user_item_matrix.shape\n",
    "    \n",
    "    for _, row in test.iterrows():\n",
    "        user_idx = min(max(int(row['user_id']) - 1, 0), num_users - 1)\n",
    "        item_idx = min(max(int(row['item_id']) - 1, 0), num_items - 1)\n",
    "        predictions.append(model.user_factors[user_idx] @ model.item_factors[item_idx].T)\n",
    "    \n",
    "    return np.clip(predictions, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Collaborative Filtering (NCF)\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=20):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim * 2, 1)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        x = torch.cat([user_emb, item_emb], dim=-1)\n",
    "        return self.fc(x).squeeze()\n",
    "\n",
    "\n",
    "def train_ncf(train, valid, num_epochs=10, embedding_dim=20):\n",
    "    num_users = int(train['user_id'].max()) + 1\n",
    "    num_items = int(train['item_id'].max()) + 1\n",
    "    model = NCF(num_users, num_items, embedding_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_users = torch.tensor(train['user_id'].values, dtype=torch.long)\n",
    "    train_items = torch.tensor(train['item_id'].values, dtype=torch.long)\n",
    "    train_ratings = torch.tensor(train['rating'].values, dtype=torch.float)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(train_users, train_items)\n",
    "        loss = criterion(predictions, train_ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: EVALUATION\n",
    "\n",
    "def evaluate(test, predictions):\n",
    "    mse = mean_squared_error(test['rating'], predictions)\n",
    "    mae = mean_absolute_error(test['rating'], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(test['rating'], predictions)\n",
    "    return mse, mae, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sara.thibierge/Library/Python/3.9/lib/python/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a213ca8c1564775baedbf495a018060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c956fa922e1049d199d7276472d9a422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Bias Correction:\n",
      "(1.7946417055866035, 0.8217243790126296, np.float64(1.339642379736698), -0.6062386757104727)\n",
      "(1.787207841873169, 0.8185670375823975, np.float64(1.3368649303026723), -0.5995852947235107)\n",
      "With Bias Correction:\n",
      "(9.016225495913346, 2.63211348382651, np.float64(3.002703031588929), -7.06969438823397)\n",
      "(1.7782549858093262, 0.8188245296478271, np.float64(1.333512274337708), -0.5915722846984863)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71826efcbe6549e7b8a94e5bb99e2818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a217ae8c95274e1fb26e4f8e0f2fcdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Bias Correction:\n",
      "(3.0494946971650925, 1.23084968994718, np.float64(1.7462802458841171), -0.9650009027960611)\n",
      "(3.080980028116043, 1.2365731047386797, np.float64(1.7552720666939479), -0.9852890849007772)\n",
      "With Bias Correction:\n",
      "(7.355411823309893, 2.308710217526781, np.float64(2.7120862492387467), -3.739601904104669)\n",
      "(3.064318243777848, 1.234049545171169, np.float64(1.7505194211370085), -0.9745527418282072)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8224c60c195416db2b05ca897e75286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70a65b1545e430793cb1c911751d372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Bias Correction:\n",
      "(5.780225450070027, 1.9822708161656433, np.float64(2.404209943010391), -2.1043167923400627)\n",
      "(5.869756698608398, 2.002556085586548, np.float64(2.422758076781171), -2.152400255203247)\n",
      "With Bias Correction:\n",
      "(6.2050329511248075, 2.086006155546017, np.float64(2.490990355486108), -2.332463093972709)\n",
      "(5.869858741760254, 2.0019917488098145, np.float64(2.422779135984181), -2.1524550914764404)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# STEP 6: EXPERIMENTATION\n",
    "# ----------------------------\n",
    "\n",
    "def run_experiment(dataset='yahoo', use_synthetic=False):\n",
    "    if use_synthetic:\n",
    "        train = generate_synthetic_data()\n",
    "        valid = generate_synthetic_data()\n",
    "        test = generate_synthetic_data()\n",
    "    elif dataset == 'yahoo':\n",
    "        train, valid, test = load_yahoo_data()\n",
    "    elif dataset == 'coat':\n",
    "        train, valid, test = load_coat_data()\n",
    "    \n",
    "    train = compute_propensities(train)\n",
    "    valid = compute_propensities(valid)\n",
    "    test = compute_propensities(test)\n",
    "    \n",
    "    best_k = 20\n",
    "    \n",
    "    svd_predictions = svd_recommender(train, test, k=best_k, debias=False)\n",
    "    svd_debias_predictions = svd_recommender(train, test, k=best_k, debias=True)\n",
    "    als_predictions = als_recommender(train, test, factors=best_k, debias=False)\n",
    "    als_debias_predictions = als_recommender(train, test, factors=best_k, debias=True)\n",
    "    \n",
    "    print(\"Without Bias Correction:\")\n",
    "    print(evaluate(test, svd_predictions))\n",
    "    print(evaluate(test, als_predictions))\n",
    "    print(\"With Bias Correction:\")\n",
    "    print(evaluate(test, svd_debias_predictions))\n",
    "    print(evaluate(test, als_debias_predictions))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment(dataset='yahoo', use_synthetic=False)\n",
    "    run_experiment(dataset='coat', use_synthetic=False)\n",
    "    run_experiment(dataset='', use_synthetic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Collaborative Filtering (NCF):\n",
      "(5.050364017486572, 1.9231197834014893, np.float64(2.2473014967926694), -3.520172595977783)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_ncf(model, test):\n",
    "    model.eval()\n",
    "    test_users = torch.tensor(test['user_id'].values, dtype=torch.long)\n",
    "    test_items = torch.tensor(test['item_id'].values, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_users, test_items).numpy()\n",
    "    return predictions\n",
    "\n",
    "def run_experiment(dataset='yahoo', use_synthetic=False):\n",
    "    if use_synthetic:\n",
    "        train = generate_synthetic_data()\n",
    "        valid = generate_synthetic_data()\n",
    "        test = generate_synthetic_data()\n",
    "    elif dataset == 'yahoo':\n",
    "        train, valid, test = load_yahoo_data()\n",
    "    elif dataset == 'coat':\n",
    "        train, valid, test = load_coat_data()\n",
    "    \n",
    "    train = compute_propensities(train)\n",
    "    valid = compute_propensities(valid)\n",
    "    test = compute_propensities(test)\n",
    "    \n",
    "    best_k = 20\n",
    "    \n",
    "    ncf_model = train_ncf(train, valid)\n",
    "    ncf_predictions = evaluate_ncf(ncf_model, test)\n",
    "    \n",
    "    print(\"Neural Collaborative Filtering (NCF):\")\n",
    "    print(evaluate(test, ncf_predictions))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment(dataset='yahoo', use_synthetic=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
